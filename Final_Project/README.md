### Final Project On Azure Machine Learning Engineering. 

In this project, I created two models, one using Automated ML and one customized model whose hyperparameters were tuned using HyperDrive. I then compared the performance of both  models and deploy the best performing model which was the model generated by the AutoML run. I was able to achieve that by importing an external dataset into my workspace and trained a model using the different tools available in the AzureML framework and also deployed the model as a web service.

## Dataset

### Overview

This dataset is about a bank (Thera Bank) which has a growing customer base. Majority of these customers are depositors with varying size of deposits. The number of customers who are asset customers are quite small and the bank is interested in expanding this base rapidly to bring in more loan business and in the process, earn more through the interest on loans. In particular, the management wants to explore ways of converting its liability customers to personal loan customers while retaining them as depositors. A campaign that the bank ran a year before for liability customers showed a healthy conversion rate of over 9% success. This has encouraged the retail marketing department to devise campaigns to better target marketing to increase the success ratio with a minimal budget.
The department wants to build a model that will help them identify the potential customers who have a higher probability of purchasing the loan. This will increase the success ratio while at the same time reduce the cost of the campaign. The dataset was obtained from the Kaggle. Follow this link for more information on the dataset https://www.kaggle.com/itsmesunil/bank-loan-modelling


Content
<table>
 <thead>
  <td>Column</td>
  <td>Description</td>
 </thead>
 <tbody>
  <tr>
   <td>
    ID </td>
   <td>Customer ID</td>
</tr>
 <tr>
        <td>Age </td>
        <td> Customer's age in completed years </td>
    </tr>
    <tr>
        <td>Experience </td>
        <td> years of professional experience </td>
    </tr>
    <tr>
        <td>Income </td>
        <td> Annual income of the customer ($000) </td>
    </tr>
    <tr>
        <td>ZIPCode </td>
        <td> Home Address ZIP code </td>
    </tr>
    <tr>
        <td>Family </td>
        <td> Family size of the customer </td>
    </tr>
    <tr>
        <td>CCAvg </td>
        <td> Avg. spending on credit cards per month ($000) </td>
    </tr>
    <tr>
        <td>Education </td>
        <td> Education Level (1: Undergrad; 2: Graduate; 3: Advanced/Professional) </td>
    </tr>
    <tr>
        <td>Mortgage </td>
        <td> Value of house mortgage if any ($000) </td>
    </tr>
    <tr>
        <td>Personal Loan </td>
        <td> Did this customer accept the personal loan offered in the last campaign? </td>
    </tr>
    <tr>
        <td>Securities Account </td>
        <td> Does the customer have a security account with the bank? </td>
    </tr>
    <tr>
        <td>CD Account </td>
        <td> Does the customer have a certificate of deposit (CD) account with the bank? </td>
    </tr>
    <tr>
        <td>Online </td>
        <td> Does the customer use internet banking facilities? </td>
    </tr>
    <tr>
        <td>CreditCard </td>
        <td> Does the customer uses a credit card issued by UniversalBank</td>
    </tr>
 </tbody>
</table>

### Task

The task i will be solving with this dataset is a classification task. The main goal of classification models is to predict which categories new data will fall into based on learnings from its training data. Therefore i will be using a classification model to predict the likelihood of a liability customer buying Personal Loan (Did this customer accept the personal loan offered in the last campaign).

### Access

To access the data, I downloaded the dataset on my system and uploaded it into my github account. I was able to access the data via a link to my github account as a 'Raw' file. I then converted the dataset into a Tabular Dataset using the TabularDatasetFactory from azureml.data.dataset_factory which is the acceptable data format for the autoML to run successfully.

## Automated ML

AutoML settings
For the AutoMLsettings, I specified an experiment_timeout_hours to 0.3 so as to terminate the experiment when this value is reached. setting enable_early stop to be true whenever the score is not improving in the short term, the iteration_timeout_minutes was set to 5 which is the maximum time for each iteration to run before it terminates. Max_concurrent_iterations was set to 4 which was the number of iterations that would be executed in parallel For the primary_metric I chose the AUC_Weighted because its better at optimizing datasets which are very small and have very large class skew (clasmay not optimize as well for datasets which are very small and have very large class skew (class imbalance). Featurization is set to 'auto'so as to allow featurization to be done automatically.

AutoML Configuration
For the AutoML Configuration, I chose the classification task because i am trying to predict which category an existing customer will fall into based on learnings from its training data. The training_data is the resulting data from the already split data after it had been converted to a tabular dataset which is what is accepted by autoML. Label_column_name is the 'Personal loan' which is the column we are trying to predict. n_cross_validation is set to 5 which is the number of cross validations to perform.

### Results

In this autoML experiment we found out VotingEnsemble to be the best model based on the AUC_Weighted metric. The AUC_Weighted score for this models was 0.99541.Other metrics scores are as follows:
Accuracy
0.98667
AUC macro
0.99541
AUC micro
0.99866
AUC weighted
0.99541
Average precision score macro
0.98645
Average precision score micro
0.99869
Average precision score weighted
0.99715
Balanced accuracy
0.93978
F1 score macro
0.95803
F1 score micro
0.98667
F1 score weighted
0.98631
Log loss
0.065460
Matthews correlation
0.91839
Norm macro recall
0.87955
Precision score macro
0.98021
Precision score micro
0.98667
Precision score weighted
0.98674
Recall score macro
0.93978
Recall score micro
0.98667
Recall score weighted
0.98667
Weighted accuracy
0.99615

Parameters:	
estimators : list of (string, estimator) tuples
Invoking the fit method on the VotingClassifier will fit clones of those original estimators that will be stored in the class attribute self.estimators_.

voting : str, {‘hard’, ‘soft’} (default=’hard’)
Else if ‘soft’, predicts the class label based on the argmax of the sums of the predicted probabilities, which is recommended for an ensemble of well-calibrated classifiers.

weights : array-like, shape = [n_classifiers], optional (default=`None`)
Sequence of weights (float or int) to weight the occurrences of predicted class labels (hard voting) or class probabilities before averaging (soft voting). Uses uniform weights if None.

n_jobs : int, optional (default=1)
The number of jobs to run in parallel for fit. If -1, then the number of jobs is set to the number of cores.

Below screenshot shows the RunDetails Widget of all models generated by our AutoML run. Displaying the best metric, status of the run, duration it takes and date it was started. 
![](https://github.com/vikkyfama/Project_Capstone/blob/toribranch/Final_Project/AutoMLFinalRunDetailsWidget.png)

Displayed in the screenshot below is a visual representation of the AutoML Run with metric: accuracy
![](https://github.com/vikkyfama/Project_Capstone/blob/toribranch/Final_Project/AutoMLFinalRunDetailsWidget2.png)

Below screenshot shows the best model gotten from the experiment as well as all the properties of the model
![](https://github.com/vikkyfama/Project_Capstone/blob/toribranch/Final_Project/AutoMLFinalRunDetailsWidget3.png)

## Hyperparameter Tuning

The model chosen for this experiment was the SKLearn LogisticRegression Algorithm. This class implements regularized logistic regression using the ‘lbfgs’ solvers. With regularization being applied by default. I chose this particular model because it can handle both dense and sparse input by using C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance.
For the early_termination_policy, The BanditPolicy was used, this defines an early termination policy based on slack criteria, and a frequency and delay interval for evaluation.
The slack_factor with a value of 0.4 simply signifies that any run that doesn't fall within this slack factor value of the evaluation metric with respect to the best performing run will be terminated.
The parameter sampler used in this experiment was the RandomParameterSampling. In this sampling, parameter values are chosen from a set of discrete values or a distribution over a continuous range. For this experiment the ranges of the values are as follows;
"learning_rate": The learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function. 
normal(10, 3)-  Returns a real value that's normally distributed with mean mu and standard deviation sigma. 

"keep_probability": This will be the probability with which we will keep each node. 
uniform(0.05, 0.1) - Returns a value uniformly distributed between 0.05 and 0.1.

"--C": Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization. 
uniform(0.02, 1)- Returns a value uniformly distributed between 0.02 and 1

"--max_iter": Maximum number of iterations taken for the solvers to converge. 
choice(50, 1000, 1500)- Specifies a discrete set of options (500, 1000, 1500) to sample from.


### Results

Accuracy result for my LogisticRegression Model was 0.9633333333333334 with the following hyperparameter values;
 Regularization Strength: 1.0
 
### Improvements
 1.  By replacing the randomparametersampling with Bayesian sampling which tries to intelligently pick the next sample of hyperparameters, based on how the previous samples performed, such that the new sample improves the reported primary metric.
 2. We could also try to change the earlyTerminationPolicy to the Truncation selection policy which will periodically cancel a given percentage of runs that rank the lowest for their performance on the primary metric. This policy strives for fairness in ranking the runs by accounting for improving model performance with training time. When ranking a relatively young run, the policy uses the corresponding (and earlier) performance of older runs for comparison. Therefore, runs aren't terminated for having a lower performance because they have run for less time than other runs.

### Hyperparameter Tuning Screenshots

Below screenshot shows the RunDetails Widget of the model generated by our hyperdrive run. Displaying the best status of the run, sttart time and duration it takes, rund_id, max_concurrent_runs and max_total_runs. 
![](https://github.com/vikkyfama/Project_Capstone/blob/toribranch/Final_Project/HyperdriveFinalRunDetailsWidget.png)

Below is displayed the best model obtained, its accuracy reading as well as the parameters and their values
![](https://github.com/vikkyfama/Project_Capstone/blob/toribranch/Final_Project/HyperdriveFinalRunDetailsWidget2.png)

Below is a screenshot of the registered model.
![](https://github.com/vikkyfama/Project_Capstone/blob/toribranch/Final_Project/hyperdriveRegistered%20model.png)


## Model Deployment

For the Model deployment, connection is firstly made to the workspace(In my case "Toria_workspace"), next is to register the model. A registered model is a logical container for one or more files that make up your model (I made use of the AutoML model in this case because it generated a more accurate model), afterwhich a script (score.py) was defined to the specifications of this particular model. The script was generated by the model and performs the task of receives data submitted to a deployed web service and passes it to the model. An inference configuration was also setup which describes how to set up the web-service containing your model. Its parameters include an entry script (score.py) and a working environment (In this case i used a curated environment). i then chose the compute target used to host my model("Vacompute"). Finally i deployed the model using the deployment_config.


Below screenshot shows the active state of the deployed model (besttoberun) with the deployment state marked as being 'Healthy' and a REST endpoint generated for making HTTP request.
![](https://github.com/vikkyfama/Project_Capstone/blob/toribranch/Final_Project/modelActive.png)


To query the endpoint with a sample input, firstly we go to the model endpoint and access the REST endpoint of the model and we will pass this as our scoring_uri on the endpoint script where we will be making the HTTP request.we will also pass the data in which must be in JSON format. The next instruction is passing the data to the model as an HTTP POST request and then getting a response. Below screenshot shows how the request was made.

![](https://github.com/vikkyfama/Project_Capstone/blob/toribranch/Final_Project/ResponseRequest.png)


### Screencast Presentation
https://youtu.be/9SSUb1Gsnu0


