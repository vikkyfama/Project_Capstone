*NOTE:* This file is a template that you can use to create the README for your project. The *TODO* comments below will highlight the information you should be sure to include.

# Your Project Title Here

*TODO:* Write a short introduction to your project.
In this project, I created two models, one using Automated ML and one customized model whose hyperparameters were tuned using HyperDrive. I then compared the performance of both  models and deploy the best performing model which was the model generated by the AutoML run. I was able to achieve that by importing an external dataset into my workspace and trained a model using the different tools available in the AzureML framework and also deployed the model as a web service.

## Project Set Up and Installation
*OPTIONAL:* If your project has any special installation steps, this is where you should put it. To turn this project into a professional portfolio project, you are encouraged to explain how to set up this project in AzureML.


## Dataset

### Overview
*TODO*: Explain about the data you are using and where you got it from.
This dataset is about a bank (Thera Bank) which has a growing customer base. Majority of these customers are depositors with varying size of deposits. The number of customers who are asset customers are quite small and the bank is interested in expanding this base rapidly to bring in more loan business and in the process, earn more through the interest on loans. In particular, the management wants to explore ways of converting its liability customers to personal loan customers while retaining them as depositors. A campaign that the bank ran a year before for liability customers showed a healthy conversion rate of over 9% success. This has encouraged the retail marketing department to devise campaigns to better target marketing to increase the success ratio with a minimal budget.
The department wants to build a model that will help them identify the potential customers who have a higher probability of purchasing the loan. This will increase the success ratio while at the same time reduce the cost of the campaign. The dataset was obtained from the Kaggle.

Content
Column Descriptions: 
ID = Customer ID, Age = Customer's age in completed years, Experience = years of professional experience, Income = Annual income of the customer ($000), ZIPCode = Home Address ZIP code, Family = Family size of the customer, CCAvg = Avg. spending on credit cards per month ($000), Education = Education Level (1: Undergrad; 2: Graduate; 3: Advanced/Professional), Mortgage = Value of house mortgage if any ($000), Personal Loan = Did this customer accept the personal loan offered in the last campaign?, Securities Account = Does the customer have a security account with the bank?, CD Account = Does the customer have a certificate of deposit (CD) account with the bank?, Online =  Does the customer use internet banking facilities?, CreditCard = Does the customer uses a credit card issued by UniversalBank.

### Task
*TODO*: Explain the task you are going to be solving with this dataset and the features you will be using for it.
The task i will be solving with this dataset is a classification task. The main goal of classification models is to predict which categories new data will fall into based on learnings from its training data. Therefore i will be using a classification model to predict the likelihood of a liability customer buying Personal Loan (Did this customer accept the personal loan offered in the last campaign).
The features of the dataset are;
Experience = years of professional experience, Income = Annual income of the customer ($000), Family = Family size of the customer, CCAvg = Avg. spending on credit cards per month ($000), Education = Education Level (1: Undergrad; 2: Graduate; 3: Advanced/Professional), Mortgage = Value of house mortgage if any ($000), Securities Account = Does the customer have a security account with the bank?, CD Account = Does the customer have a certificate of deposit (CD) account with the bank?, Online =  Does the customer use internet banking facilities?, CreditCard = Does the customer uses a credit card issued by UniversalBank.




### Access
*TODO*: Explain how you are accessing the data in your workspace.
To access the data, I downloaded the dataset on my system and uploaded it into my github account. I was able to access the data via a link to my github account as a 'Raw' file. I then converted the dataset into a Tabular Dataset using the TabularDatasetFactory from azureml.data.dataset_factory which is the acceptable data format for the autoML to run successfully.

## Automated ML
*TODO*: Give an overview of the `automl` settings and configuration you used for this experiment
AutoML settings
For the AutoMLsettings, I specified an experiment_timeout_hours to 0.3 so as to terminate the experiment when this value is reached. setting enable_early stop to be true whenever the score is not improving in the short term, the iteration_timeout_minutes was set to 5 which is the maximum time for each iteration to run before it terminates. Max_concurrent_iterations was set to 4 which was the number of iterations that would be executed in parallel For the primary_metric I chose the AUC_Weighted because its better at optimizing datasets which are very small and have very large class skew (clasmay not optimize as well for datasets which are very small and have very large class skew (class imbalance). Featurization is set to 'auto'so as to allow featurization to be done automatically.

AutoML Configuration
For the AutoML Configuration, I chose the classification task because i am trying to predict which category an existing customer will fall into based on learnings from its training data. The training_data is the resulting data from the already split data after it had been converted to a tabular dataset which is what is accepted by autoML. Label_column_name is the 'Personal loan' which is the column we are trying to predict. n_cross_validation is set to 5 which is the number of cross validations to perform.

### Results
*TODO*: What are the results you got with your automated ML model? What were the parameters of the model? How could you have improved it?
In this autoML experiment we found out VotingEnsemble to be the best model based on the AUC_Weighted metric. The AUC_Weighted score for this models was 0.99541.Other metrics scores are as follows:
Accuracy
0.98667
AUC macro
0.99541
AUC micro
0.99866
AUC weighted
0.99541
Average precision score macro
0.98645
Average precision score micro
0.99869
Average precision score weighted
0.99715
Balanced accuracy
0.93978
F1 score macro
0.95803
F1 score micro
0.98667
F1 score weighted
0.98631
Log loss
0.065460
Matthews correlation
0.91839
Norm macro recall
0.87955
Precision score macro
0.98021
Precision score micro
0.98667
Precision score weighted
0.98674
Recall score macro
0.93978
Recall score micro
0.98667
Recall score weighted
0.98667
Weighted accuracy
0.99615

Parameters:	
estimators : list of (string, estimator) tuples
Invoking the fit method on the VotingClassifier will fit clones of those original estimators that will be stored in the class attribute self.estimators_.

voting : str, {‘hard’, ‘soft’} (default=’hard’)
Else if ‘soft’, predicts the class label based on the argmax of the sums of the predicted probabilities, which is recommended for an ensemble of well-calibrated classifiers.

weights : array-like, shape = [n_classifiers], optional (default=`None`)
Sequence of weights (float or int) to weight the occurrences of predicted class labels (hard voting) or class probabilities before averaging (soft voting). Uses uniform weights if None.

n_jobs : int, optional (default=1)
The number of jobs to run in parallel for fit. If -1, then the number of jobs is set to the number of cores.

*TODO* Remeber to provide screenshots of the `RunDetails` widget as well as a screenshot of the best model trained with it's parameters.
![](https://github.com/vikkyfama/Project_Capstone/blob/toribranch/Final_Project/AutoMLFinalRunDetailsWidget.png)
![](https://github.com/vikkyfama/Project_Capstone/blob/toribranch/Final_Project/AutoMLFinalRunDetailsWidget2.png)
![](https://github.com/vikkyfama/Project_Capstone/blob/toribranch/Final_Project/AutoMLFinalRunDetailsWidget3.png)

## Hyperparameter Tuning
*TODO*: What kind of model did you choose for this experiment and why? Give an overview of the types of parameters and their ranges used for the hyperparameter search
The model chosen for this experiment was the SKLearn LogisticRegression Algorithm. This class implements regularized logistic regression using the ‘lbfgs’ solvers. With regularization being applied by default. I chose this particular model because it can handle both dense and sparse input by using C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance.
For the early_termination_policy, The BanditPolicy was used, this defines an early termination policy based on slack criteria, and a frequency and delay interval for evaluation.
The slack_factor with a value of 0.4 simply signifies that any run that doesn't fall within this slack factor value of the evaluation metric with respect to the best performing run will be terminated.
The parameter sampler used in this experiment was the RandomParameterSampling. In this sampling, parameter values are chosen from a set of discrete values or a distribution over a continuous range. For this experiment the ranges of the values are as follows;
"learning_rate": The learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function. 
normal(10, 3)-  Returns a real value that's normally distributed with mean mu and standard deviation sigma. 

"keep_probability": This will be the probability with which we will keep each node. 
uniform(0.05, 0.1) - Returns a value uniformly distributed between 0.05 and 0.1.

"--C": Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization. 
uniform(0.02, 1)- Returns a value uniformly distributed between 0.02 and 1

"--max_iter": Maximum number of iterations taken for the solvers to converge. 
choice(50, 1000, 1500)- Specifies a discrete set of options (500, 1000, 1500) to sample from.


### Results
*TODO*: What are the results you got with your model? What were the parameters of the model? How could you have improved it?
Accuracy result for my LogisticRegression Model was 0.9633333333333334 with the following hyperparameter values;
 Regularization Strength: 1.0
 
### Improvements
 1.  By replacing the randomparametersampling with Bayesian sampling which tries to intelligently pick the next sample of hyperparameters, based on how the previous samples performed, such that the new sample improves the reported primary metric.
 2. We could also try to change the earlyTerminationPolicy to the Truncation selection policy which will periodically cancel a given percentage of runs that rank the lowest for their performance on the primary metric. This policy strives for fairness in ranking the runs by accounting for improving model performance with training time. When ranking a relatively young run, the policy uses the corresponding (and earlier) performance of older runs for comparison. Therefore, runs aren't terminated for having a lower performance because they have run for less time than other runs.

*TODO* Remeber to provide screenshots of the `RunDetails` widget as well as a screenshot of the best model trained with it's parameters.

## Model Deployment
*TODO*: Give an overview of the deployed model and instructions on how to query the endpoint with a sample input.

## Screen Recording
*TODO* Provide a link to a screen recording of the project in action. Remember that the screencast should demonstrate:
- A working model
- Demo of the deployed  model
- Demo of a sample request sent to the endpoint and its response

## Standout Suggestions
*TODO (Optional):* This is where you can provide information about any standout suggestions that you have attempted.
